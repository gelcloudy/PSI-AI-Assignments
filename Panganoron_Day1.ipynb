{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BxCbyGKLsDIUVop5lKE1tnecThArc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT-Index) are both powerful frameworks used for building applications that leverage large language models (LLMs), but they serve different primary purposes and have distinct design philosophies.\\n\\n**LangChain**\\n\\n- **Purpose:** A comprehensive framework for developing LLM-powered applications, especially those involving complex chains of prompts, tools, memory, and retrieval components.\\n- **Core Focus:**\\n  - Building versatile chains of LLM calls, including prompt engineering, conversational agents, question answering, and more.\\n  - Integrating with various data sources and tools.\\n  - Facilitating state management across interactions (memory).\\n  - Supporting a modular pipeline approach—prompt templates, chains, and tools.\\n- **Features:**\\n  - Supports multiple LLM providers and models.\\n  - Rich ecosystem for prompt management.\\n  - Supports memory and state management.\\n  - Extensibility for custom chains and tools.\\n- **Use Cases:** Chatbots, virtual assistants, multi-step reasoning, complex workflows involving LLMs.\\n\\n---\\n\\n**LlamaIndex (GPT-Index)**\\n\\n- **Purpose:** A specialized framework for connecting large language models with external data sources, focusing heavily on data ingestion, indexing, and retrieval.\\n- **Core Focus:**\\n  - Building indexes over various data sources such as documents, PDFs, databases, and APIs.\\n  - Facilitating efficient retrieval-augmented generation (RAG) workflows.\\n  - Making it easier to query large, unstructured or semi-structured data using LLMs.\\n- **Features:**\\n  - Multiple data importers and index types.\\n  - Hierarchical indexing structures.\\n  - Simplifies the process of querying external knowledge bases.\\n  - Designed for data-driven applications where retrieval speed and accuracy matter.\\n- **Use Cases:** Document QA systems, knowledge bases, search engines powered by LLMs, data augmentation for LLM tasks.\\n\\n---\\n\\n### Summary\\n\\n| Aspect                      | **LangChain**                                     | **LlamaIndex**                                 |\\n|-----------------------------|--------------------------------------------------|------------------------------------------------|\\n| Main Purpose                | General-purpose LLM application framework        | Data ingestion, indexing, and retrieval       |\\n| Focus                       | Chain orchestration, prompt management, tools   | Efficient retrieval from external data sources |\\n| Data Handling               | Not primarily focused on data indexing           | Central to its design—builds indexes over data  |\\n| Use Cases                   | Conversational agents, multi-step workflows     | Document QA, knowledge bases, retrieval tasks  |\\n| Extensibility               | Highly modular, supports various integrations   | Optimized for data-indexed retrieval tasks    |\\n\\n**In essence:**  \\n- Use **LangChain** if you're building complex LLM applications involving multiple steps, memory, tools, and chains.  \\n- Use **LlamaIndex** if your main goal is to connect an LLM with a large corpus of external data and enable efficient retrieval and question answering over that data.\\n\\nBoth can be complementary—some projects combine both frameworks: LangChain for orchestration and tools, and LlamaIndex for data indexing and retrieval.\\n\\n---\\n\\nIf you'd like more details or examples of each, feel free to ask!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753449530, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=643, prompt_tokens=19, total_tokens=662, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks in the AI/ML ecosystem, especially for building applications that involve large language models (LLMs) and integrating external data sources. However, they serve different primary purposes and have distinct features. Here's a breakdown of their differences:\n",
              "\n",
              "**1. Primary Purpose:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Focuses on building **AI-powered applications** that involve complex interactions with LLMs.\n",
              "  - Provides **tools and abstractions** for chains, prompts, memory, agents, and integrations.\n",
              "  - Enables developers to create **conversational agents, multi-step workflows, and dynamic LLM interactions**.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Designed to facilitate **easy indexing and querying of external data sources** using LLMs.\n",
              "  - Focuses on **building custom knowledge bases** from documents, databases, or other data repositories.\n",
              "  - Simplifies the process of **retrieving relevant information** for question-answering or information extraction.\n",
              "\n",
              "**2. Core Functionality:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Chains and pipelines to orchestrate multiple steps in LLM tasks.\n",
              "  - Memory management for stateful conversations.\n",
              "  - Tool use and agent frameworks to enable LLMs to interact with APIs, databases, or custom tools.\n",
              "  - Supports various language models and providers.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Data ingestion, indexing, and retrieval over large document datasets.\n",
              "  - Built-in support for vector similarity search, keyword search, and other retrieval methods.\n",
              "  - Focused on **building and querying large external knowledge bases** efficiently.\n",
              "\n",
              "**3. Use Cases:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Chatbots with context retention.\n",
              "  - Multi-step AI workflows.\n",
              "  - Autonomous agents that can call APIs or perform actions.\n",
              "  - Complex prompt engineering.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Building a custom question-answering system over your company's documents.\n",
              "  - Creating a knowledge base from a corpus of PDFs, text files, or web data.\n",
              "  - Efficient retrieval of relevant information to supplement LLM responses.\n",
              "\n",
              "**4. Integration and Extensibility:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Extensive integrations with LLM providers (OpenAI, Azure, Hugging Face, etc.).\n",
              "  - Supports a variety of tools, prompts, and memory mechanisms.\n",
              "  - Highly modular and customizable.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focuses on data connectors for various data sources.\n",
              "  - Supports multiple retrieval and indexing strategies.\n",
              "  - Less emphasis on conversational workflows, more on data management.\n",
              "\n",
              "---\n",
              "\n",
              "### In summary:\n",
              "\n",
              "| Aspect | **LangChain** | **LlamaIndex** |\n",
              "|---------|----------------|----------------|\n",
              "| Focus | Building conversational AI, complex LLM workflows | Building, indexing, and querying knowledge bases |\n",
              "| Core use cases | Chatbots, agents, multi-step prompts | Data indexing, retrieval, question-answering over documents |\n",
              "| Main features | Chains, agents, memory, tool integrations | Data ingestion, vector search, document indexing |\n",
              "| Integration scope | Broad LLM and tool integrations | Data sources, retrieval techniques |\n",
              "\n",
              "**In essence**, choose **LangChain** if you need to orchestrate complex LLM interactions, build chatbots, or sophisticated AI workflows, and choose **LlamaIndex** if your goal is to create a searchable index of external data for efficient question-answering or information retrieval.\n",
              "\n",
              "---\n",
              "\n",
              "If you'd like, I can also help you understand how these frameworks might be used together or provide specific examples!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time for your silly questions when I'm starving and furious! Let's get to something that actually matters—like how to get some food before I completely lose it!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is great for making drinks extra refreshing and cool, while cubed ice is perfect for keeping beverages cold without diluting them too quickly. Both have their own charm—what’s your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BxQB9U5rYbvJM3e7nA4WXv1lwhpUs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice is great for making drinks extra refreshing and cool, while cubed ice is perfect for keeping beverages cold without diluting them too quickly. Both have their own charm—what’s your favorite?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753501683, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=42, prompt_tokens=30, total_tokens=72, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term alterations in Earth's climate system, primarily driven by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase greenhouse gases like carbon dioxide and methane in the atmosphere, trapping more heat and leading to global warming. The consequences include rising sea levels, more extreme weather events, melting glaciers, and threats to biodiversity and agriculture. Addressing climate change requires reducing emissions, transitioning to renewable energy sources, and adopting sustainable practices worldwide to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Alam niyo, mga kaibigan, ang climate change parang love life lang yan — minsan maganda, minsan nakakaloka! Pero seryoso, ha, ang climate change ay seryosong usapin na kailangang pagtuunan ng pansin. Dahil habang tumatagal, nakakaramdam tayo ng mas matinding init, bagyo, at baha. Parang sa buhay, dapat maging responsable tayo sa ating mga actions. Kasi kung hindi, tulad ng climate change, ang ending, tayo rin ang kawawa. Kaya’t huwag kalimutan, magtulungan tayo—dahil ang tunay na pagbabago, nagsisimula sa atin. Paalam na muna, at maging mapagmatyag sa kalikasan!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly turned the falbean, securing the parts firmly in place."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the word \"strawberry\" letter by letter:\n",
              "\n",
              "s ✓ (no)\n",
              "t ✓ (no)\n",
              "r ✓ (yes)\n",
              "a ✓ (no)\n",
              "w ✓ (no)\n",
              "b ✓ (no)\n",
              "e ✓ (no)\n",
              "r ✓ (yes)\n",
              "r ✓ (yes)\n",
              "y ✓ (no)\n",
              "\n",
              "**Number of 'r's:** 3\n",
              "\n",
              "Here is each letter with a checkmark if it is 'r':\n",
              "\n",
              "- s ✓\n",
              "- t\n",
              "- r ✓\n",
              "- a\n",
              "- w\n",
              "- b\n",
              "- e\n",
              "- r ✓\n",
              "- r ✓\n",
              "- y\n",
              "\n",
              "**Total 'r's:** 3"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry? write down each letter and put a checkmark if it is the letter r\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
